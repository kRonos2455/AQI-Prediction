{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "039553f3-edd9-4866-bd92-7c790494902c",
   "metadata": {},
   "source": [
    "## **Preprocess the Web Scrapped Data and Combine with PM2.5 Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46fcf3d6-47c9-4a1c-8861-fbf89bc33e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd.set_option('display.max_rows', 500)\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from bs4 import BeautifulSoup\n",
    "import os \n",
    "import sys\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "752f04c3-c453-4736-8341-26eaeec77959",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('Data_blr_tutiempo/Html_data/2018/1.html', 'rb').read()\n",
    "texts = BeautifulSoup(file, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01835843-c5da-47d4-b5e8-438539f04363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(texts.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87732a2d-00e4-42c5-927f-62d96e2c2a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T', 'TM', 'Tm', 'H', 'VV', 'V', 'VM', 'PM2.5']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = [header.text for header in texts.find_all('th')]\n",
    "headers.pop(15)\n",
    "headers.pop(14)\n",
    "headers.pop(13)\n",
    "headers.pop(12)\n",
    "headers.pop(11)\n",
    "headers.pop(10)\n",
    "headers.pop(6)\n",
    "headers.pop(0)\n",
    "headers.pop(3)\n",
    "headers.append('PM2.5')\n",
    "\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3af4c716-7c41-49b5-93d8-5ddc23579199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = list()\n",
    "# for tables in texts.findAll('table', {'class': \"medias mensuales numspan\"}):\n",
    "#     for rows in tables.find_all('tr'):\n",
    "#         cols = rows.find_all('td')\n",
    "#         # Day = rows[0].text\n",
    "#         if len(cols)!=0:\n",
    "#             day = cols[0].text\n",
    "#             T = cols[1].text\n",
    "#             TM = cols[1].text\n",
    "#             Tm = cols[1].text\n",
    "#             SLP = cols[1].text\n",
    "#             H = cols[1].text\n",
    "#             PP = cols[1].text\n",
    "#             VV = cols[1].text\n",
    "#             V= cols[1].text\n",
    "#             VM= cols[1].text\n",
    "#             VG= cols[1].text\n",
    "#             RA= cols[1].text\n",
    "#             SN= cols[1].text\n",
    "#             TS= cols[1].text\n",
    "#             FG= cols[1].text\n",
    "#             a.append([col.text for col in cols])\n",
    "# pd.DataFrame(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288908bb-fbfc-418a-88a0-10faa5a59e0b",
   "metadata": {},
   "source": [
    "**Upon inspection, it was noticed that several table entries (corresponding to the flag `td`) were not getting parsed, which resulted in some data loss. The data can be viewed on website [(Link)](https://en.tutiempo.net/climate/01-2018/ws-432950.html) but on inspecting the webpage (F12) the table entries are encrypted using the `span` html tag. This encryption pattern is same for all entries in month.\n",
    "Since we are collecting data from 2018 - 2023 (6 years => 72 months), there could be 72 different encryption patterns. \n",
    "Hence, to avoid complexity, I have removed the table entries which cannot be parsed.\n",
    "One example of the encryption is given using the below code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b653dfd9-5607-4b8c-b695-ef645bff64bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_dictionaries = {1: \n",
    "                       {\n",
    "                           'nttu':'2',\n",
    "                           'ntlm':'1',\n",
    "                           'ntyc':'.',\n",
    "                           'nthj':'6',\n",
    "                           'ntfs':'7',\n",
    "                           'ntzb':'5',\n",
    "                           'ntas':'8',\n",
    "                           'nttn':'9',\n",
    "                           'ntbb':'4',\n",
    "                           'ntcd':'3',\n",
    "                           'ntde':'0',\n",
    "                           'ntio':'-',\n",
    "                           '&nbsp': \" \"\n",
    "                       }, \n",
    "                   2: {\n",
    "                            'ntrs': '2',\n",
    "                            'ntza': '3',\n",
    "                            'ntvr': '.',\n",
    "                            'ntbc': '0',\n",
    "                            'ntqr': '8', \n",
    "                            'ntjk': '1', \n",
    "                            'ntzz': '-', \n",
    "                            'ntgy': '6', \n",
    "                            'ntbz': '5', \n",
    "                            'ntox': '7', \n",
    "                            'ntnt': '9', \n",
    "                            'ntaa': '4'\n",
    "                        }, \n",
    "                   3: {\n",
    "                            'ntpq': '2', \n",
    "                            'ntxo': '7', \n",
    "                            'ntux': '.', \n",
    "                            'ntgg': '4', \n",
    "                            'ntxy': '3', \n",
    "                            'nthi': '1',\n",
    "                            'ntre': '8', \n",
    "                            'ntyy': '6', \n",
    "                            'ntll': '9', \n",
    "                            'ntaz': '5',\n",
    "                            'ntab': '0', \n",
    "                            'ntkf': '-'\n",
    "                       }, \n",
    "                   4: {\n",
    "                            'ntvw': '2', \n",
    "                            'ntef': '3', \n",
    "                            'ntkk': '.', \n",
    "                            'ntno': '1', \n",
    "                            'ntjj': '-', \n",
    "                            'ntdr': '6', \n",
    "                            'nthy': '8', \n",
    "                            'ntpo': '7', \n",
    "                            'ntjg': '9', \n",
    "                            'ntgo': '5', \n",
    "                            'ntee': '4', \n",
    "                            'ntfg': '0'\n",
    "                   }\n",
    "                  }\n",
    "\n",
    "# span_mapping_date = {\n",
    "#     1: [\"2018_1\", \"2018_2\" \"2018_3\", \"2018_12\", \"2019_1\", \"2019_8\", \"2019_9\", \"2020_1\", \"2020_4\", \"2020_6\", \"2020_7\", \"2020_9\", \"2020_10\", \"2021_2\", \"2021_7\", \"2021_8\", \"2021_12\", \"2022_1\", \"2022_4\", \"2022_11\", \"2023_4\"],\n",
    "#     2: [\"2018_5\", \"2018_6\", \"2018_7\", \"2018_10\", \"2018_11\", \"2019_2\", \"2019_4\", \"2019_5\", \"2019_12\", \"2020_3\", \"2020_5\", \"2021_1\", \"2021_3\", \"2021_4\", \"2021_5\", \"2021_9\", \"2021_10\", \"2021_11\", \"2022_2\", \"2022_7\", \"2022_8\", \"2022_9\", \"2023_3\", \"2023_6\"],\n",
    "#     3: [\"2018_4\", \"2018_8\", \"2019_10\", \"2019_7\", \"2020_2\", \"2021_5\", \"2022_6\", \"2022_10\", \"2022_12\", \"2023_8\", \"2023_9\", \"2023_10\"],\n",
    "#     4: [\"2019_3\", \"2019_6\", \"2019_11\", \"2020_8\", \"2020_11\", \"2020_12\", \"2022_3\", \"2022_5\", \"2023_1\", \"2023_2\", \"2023_5\", \"2023_7\", \"2023_11\", \"2023_12\"]\n",
    "# }\n",
    "\n",
    "# def find_key_by_value(value, dictionary = span_mapping_date):\n",
    "#     for key, val in dictionary.items():\n",
    "#         if value in val:\n",
    "#             return key\n",
    "\n",
    "def find_encoding(sample_encodings, dictionary = span_dictionaries): # sample_encodings = ['ntrs']\n",
    "    for keys, vals in dictionary.items():            \n",
    "        if sample_encodings[0] in list(vals.keys()):\n",
    "            return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "344436af-fee1-4029-872e-8b31cd5d608f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# span_dictionaries[find_key_by_value(\"2018_7\")]['ntrs']\n",
    "find_encoding(['ntrs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fba4f3c-5097-41e7-9de7-a81aaa7f2564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def met_data(month, year):\n",
    "    file = open(f'Data_blr_tutiempo/Html_data/{year}/{month}.html', 'rb').read()\n",
    "    texts = BeautifulSoup(file, 'lxml')\n",
    "\n",
    "    # The encodings change everytime we scrap data from the website so we have to select the encoding before decrypting the data\n",
    "    for rows in texts.find('table').find_all('tr'):\n",
    "        cols = rows.find_all('td')\n",
    "        col = list()\n",
    "        if len(cols)!=0:\n",
    "            for item in cols:\n",
    "                if len(item.text)==0:\n",
    "                    spans = [a.get('class')[0].strip() for a in item.find_all('span')]\n",
    "                    # print(spans)\n",
    "                    span_dictionary = span_dictionaries[find_encoding(spans)]\n",
    "                    break\n",
    "\n",
    "    data = list()\n",
    "    finalD = list()\n",
    "    \n",
    "    # span_encoding = str(year)+\"_\"+str(month)\n",
    "    # span_dictionary = span_dictionaries[find_key_by_value(span_encoding)]\n",
    "    \n",
    "    for rows in texts.find('table').find_all('tr'):\n",
    "        cols = rows.find_all('td')\n",
    "        col = list()\n",
    "        if len(cols)!=0:\n",
    "            for item in cols:\n",
    "                if len(item.text)!=0:\n",
    "                    col.append(item.text)\n",
    "    \n",
    "                else:\n",
    "                    col.append(\"\".join([span_dictionary[a.get('class')[0].strip()] for a in item.find_all('span')]))\n",
    "\n",
    "            finalD.append(col)\n",
    "\n",
    "    length = len(finalD)\n",
    "\n",
    "    finalD.pop(length-1)\n",
    "        \n",
    "    for a in range(len(finalD)):\n",
    "        finalD[a].pop(14)\n",
    "        finalD[a].pop(13)\n",
    "        finalD[a].pop(12)\n",
    "        finalD[a].pop(11)\n",
    "        finalD[a].pop(10)\n",
    "        finalD[a].pop(6)\n",
    "        finalD[a].pop(4)\n",
    "        finalD[a].pop(0)\n",
    "        \n",
    "    return finalD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41fb0dd5-0507-445c-8521-9fc80cb10f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['22.2', '28.4', '16.6', '53', '7.7', '0.9', '5.4'],\n",
       " ['22.1', '28.9', '17.8', '58', '5.1', '1.1', '-'],\n",
       " ['22.2', '29', '18.4', '62', '6', '3.9', '5.4'],\n",
       " ['21.6', '27.6', '17', '61', '6.3', '5', '7.6'],\n",
       " ['20.7', '28.3', '15.5', '54', '6.3', '4.8', '7.6'],\n",
       " ['20.2', '27.2', '14.7', '52', '5.8', '2.8', '5.4'],\n",
       " ['20.4', '27.2', '15.2', '52', '6.3', '4.1', '7.6'],\n",
       " ['20.1', '27.2', '14.8', '50', '6.9', '6.5', '7.6'],\n",
       " ['21.2', '26.6', '15.5', '59', '6.3', '6.7', '9.4'],\n",
       " ['21.8', '27.1', '18.2', '66', '5.8', '5.7', '11.1'],\n",
       " ['22.7', '28.3', '17.3', '65', '6.9', '4.8', '9.4'],\n",
       " ['23.1', '29.4', '18.7', '64', '6.3', '3.9', '7.6'],\n",
       " ['23.4', '30.6', '17.6', '48', '6.3', '4.1', '5.4'],\n",
       " ['23.3', '31', '16.3', '42', '5.8', '2.8', '11.1'],\n",
       " ['22.4', '30.7', '16.3', '49', '6.3', '3.3', '7.6'],\n",
       " ['22.9', '30.5', '17.1', '49', '6.3', '3.9', '7.6'],\n",
       " ['21.8', '29.9', '16.8', '49', '5.5', '5.6', '7.6'],\n",
       " ['21.2', '28.6', '15.3', '41', '6.3', '6.7', '11.1'],\n",
       " ['20.7', '28.7', '14.3', '48', '6.9', '7.2', '9.4'],\n",
       " ['20.4', '28', '14.2', '45', '6.3', '4.6', '7.6'],\n",
       " ['20.9', '28.5', '14.3', '46', '6.3', '4.1', '7.6'],\n",
       " ['20.4', '29', '14.4', '48', '5.6', '4.6', '7.6'],\n",
       " ['22.1', '28.8', '15.3', '57', '6', '2.8', '5.4'],\n",
       " ['22.4', '29.7', '17.2', '59', '6.3', '2.2', '11.1'],\n",
       " ['22.7', '29.1', '17.1', '48', '6.3', '2', '5.4'],\n",
       " ['21.9', '29.8', '16.2', '44', '6.3', '2.6', '7.6'],\n",
       " ['21.3', '29', '16', '53', '6.9', '4.4', '11.1'],\n",
       " ['21.9', '28.2', '16.5', '50', '6.3', '5.4', '11.1'],\n",
       " ['22', '29.2', '16', '50', '6.9', '5.9', '7.6'],\n",
       " ['21.5', '29.5', '15.7', '47', '5.6', '3.9', '11.1'],\n",
       " ['21.4', '29.2', '15', '42', '6.9', '5.2', '9.4']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "met_data(1, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0de70818-b610-468c-910b-5be2142bca06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To check if the encodings we have selected are correctly decoding the data or not\n",
    "for year in range(2018, 2024):\n",
    "    for month in range(1, 13):\n",
    "        try:\n",
    "            met_data(month, year)\n",
    "        except:\n",
    "            print(month, year)\n",
    "        # else:\n",
    "        #     met_data(month, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "545d1615-8795-4bf0-965c-65d5c56c5738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(met_data(1, 2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "857d3af3-3bd0-4650-b876-defc45c5e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def met_data(month, year):\n",
    "#     '''\n",
    "#     This function parses the html data (atmospheric data) collected previously and also removed features/columns which contains \n",
    "#     null values ('-', \" \") based on the year and month provided.\n",
    "#     '''\n",
    "#     file_path = f'Data_blr_tutiempo/Html_data/{year}/{month}.html'\n",
    "#     file_html = open(file_path, 'rb')\n",
    "#     plain_text = file_html.read()\n",
    "\n",
    "#     tempD = list()\n",
    "#     finalD = list()\n",
    "\n",
    "#     soup = BeautifulSoup(plain_text, 'lxml')\n",
    "\n",
    "#     for table in soup.find_all('table', {'class': 'medias mensuales numspan'}):       \n",
    "#         rows = table.find_all('tr')        \n",
    "        \n",
    "#         for row in rows:\n",
    "#             cols = row.find_all('td')\n",
    "#             if len(cols)!=0:\n",
    "#                 finalD.append([a.text for a in cols]) \n",
    "\n",
    "#     length = len(finalD)\n",
    "\n",
    "#     finalD.pop(length-1)\n",
    "    \n",
    "#     for a in range(len(finalD)):\n",
    "#         finalD[a].pop(6) # PP\n",
    "#         finalD[a].pop(13) # \n",
    "#         finalD[a].pop(12)\n",
    "#         finalD[a].pop(11)\n",
    "#         finalD[a].pop(10)\n",
    "#         finalD[a].pop(9)\n",
    "#         finalD[a].pop(0)\n",
    "#         finalD[a].pop(3) # SLP\n",
    "    \n",
    "#     return finalD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef801f6e-6372-43d5-80ce-eba25c4911e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['24.2', '29.5', '20.6', '75', '6.9', '9.4', '14.8'],\n",
       " ['24.8', '29.4', '20.9', '68', '6.9', '10.9', '14.8'],\n",
       " ['24.4', '30.2', '20.7', '68', '6.9', '8.1', '11.1'],\n",
       " ['24.1', '29', '20.5', '67', '6.9', '11.1', '18.3'],\n",
       " ['24.5', '29.6', '19.7', '65', '6.9', '9.3', '14.8'],\n",
       " ['22.8', '29.6', '20', '80', '6.9', '10.4', '18.3'],\n",
       " ['22.9', '26.3', '20.5', '82', '6.9', '11.3', '16.5'],\n",
       " ['22.4', '26.4', '20.3', '86', '8', '6.7', '9.4'],\n",
       " ['22.8', '26.4', '20.7', '85', '6.9', '8.7', '14.8'],\n",
       " ['22.7', '26.9', '20.6', '86', '7.4', '9.3', '13'],\n",
       " ['22.5', '27', '20.3', '87', '6.3', '9.3', '14.8'],\n",
       " ['22.8', '27.3', '20.1', '80', '6.9', '11.7', '14.8'],\n",
       " ['22.2', '27.4', '20.5', '86', '7.7', '10.9', '14.8'],\n",
       " ['22.4', '26.5', '20.3', '86', '6.9', '8.5', '11.1'],\n",
       " ['23.6', '27.2', '20.4', '74', '6.9', '10', '13'],\n",
       " ['22.2', '28.4', '20.2', '79', '6.9', '13.9', '18.3'],\n",
       " ['22.4', '26.2', '19.8', '79', '6.9', '12.2', '18.3'],\n",
       " ['23.8', '28', '19.7', '74', '6.9', '12.8', '18.3'],\n",
       " ['23', '28.6', '21', '79', '6.9', '14.8', '25.9'],\n",
       " ['23', '27', '20.4', '77', '6.9', '14.6', '22.2'],\n",
       " ['22.7', '26.9', '20.3', '83', '8.5', '14.8', '18.3'],\n",
       " ['23.6', '27.8', '20.3', '74', '6.9', '12', '14.8'],\n",
       " ['22.7', '28.3', '20.4', '82', '6.3', '9.6', '14.8'],\n",
       " ['23.1', '27.6', '20.3', '84', '6.9', '9.3', '11.1'],\n",
       " ['23.3', '28.2', '20.2', '82', '6.9', '9.1', '14.8'],\n",
       " ['23.7', '28.8', '20.6', '79', '6.9', '8.9', '11.1'],\n",
       " ['23.3', '27.3', '20.5', '76', '6.9', '10.9', '16.5'],\n",
       " ['23.5', '27.3', '20', '82', '6.9', '9.4', '14.8'],\n",
       " ['23.8', '27.4', '20.7', '78', '6.9', '6.7', '11.1'],\n",
       " ['23.8', '27.8', '20.5', '78', '6.9', '8.3', '11.1'],\n",
       " ['23.1', '28.3', '20.3', '81', '6.9', '10', '22.2']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "met_data(7, 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ff10f-f4db-4241-bcbd-9d8a0410a335",
   "metadata": {},
   "source": [
    "> Below code is used to create the final dataset where we merge the AQI data (dependent variable) and atmospheric data (independent variables; web scraped html data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3e1cae3-ce96-42a3-abdd-1acef6a5fe10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 1\n",
      "2018 2\n",
      "2018 3\n",
      "2018 4\n",
      "2018 5\n",
      "2018 6\n",
      "2018 7\n",
      "2018 8\n",
      "2018 9\n",
      "2018 10\n",
      "2018 11\n",
      "2018 12\n",
      "2019 1\n",
      "2019 2\n",
      "2019 3\n",
      "2019 4\n",
      "2019 5\n",
      "2019 6\n",
      "2019 7\n",
      "2019 8\n",
      "2019 9\n",
      "2019 10\n",
      "2019 11\n",
      "2019 12\n",
      "2020 1\n",
      "2020 2\n",
      "2020 3\n",
      "2020 4\n",
      "2020 5\n",
      "2020 6\n",
      "2020 7\n",
      "2020 8\n",
      "2020 9\n",
      "2020 10\n",
      "2020 11\n",
      "2020 12\n",
      "2021 1\n",
      "2021 2\n",
      "2021 3\n",
      "2021 4\n",
      "2021 5\n",
      "2021 6\n",
      "2021 7\n",
      "2021 8\n",
      "2021 9\n",
      "2021 10\n",
      "2021 11\n",
      "2021 12\n",
      "2022 1\n",
      "2022 2\n",
      "2022 3\n",
      "2022 4\n",
      "2022 5\n",
      "2022 6\n",
      "2022 7\n",
      "2022 8\n",
      "2022 9\n",
      "2022 10\n",
      "2022 11\n",
      "2022 12\n",
      "2023 1\n",
      "2023 2\n",
      "2023 3\n",
      "2023 4\n",
      "2023 5\n",
      "2023 6\n",
      "2023 7\n",
      "2023 8\n",
      "2023 9\n",
      "2023 10\n",
      "2023 11\n",
      "2023 12\n"
     ]
    }
   ],
   "source": [
    "start_year = 2018\n",
    "end_year = 2023\n",
    "if not os.path.exists('Final_dataset/Final_data'):\n",
    "    os.makedirs('Final_dataset/Final_data')\n",
    "    \n",
    "for year in range(start_year, end_year+1):\n",
    "    final_data = list()\n",
    "    \n",
    "    with open(f'Final_dataset/Final_data/{year}.csv', 'w', encoding='utf-8') as fp:\n",
    "        wr = csv.writer(fp, dialect='excel')\n",
    "        wr.writerow(headers)\n",
    "\n",
    "    with open(f'Data_blr_AQI/{year}.csv', 'r', encoding='utf-8') as aqi:\n",
    "        aqi_data = csv.reader(aqi)\n",
    "        content = [row[1] for row in aqi_data]\n",
    "        content.pop(0)\n",
    "    \n",
    "    for month in range(1, 13):\n",
    "        print(year, month)\n",
    "        temp = met_data(month, year)\n",
    "        final_data += temp\n",
    "\n",
    "    # Merge the AQI and web scraped data\n",
    "    for day in range(0, 365):\n",
    "        final_data[day].insert(8, content[day])\n",
    "        # print(final_data[day])\n",
    "\n",
    "    \n",
    "    with open(f'Final_dataset/Final_data/{year}.csv', 'a', encoding='utf-8') as fp:\n",
    "        wr = csv.writer(fp, dialect='excel')\n",
    "        for row in final_data:\n",
    "            flag=0\n",
    "            for elem in row:\n",
    "                if elem==\"\" or elem==\"-\":\n",
    "                    flag=1 # Many rows contains empty values; Inorder to remove them\n",
    "            if flag!=1:\n",
    "                wr.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac690d24-e34f-472d-8e43-2f66d9dbabcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_combine(year, cs): # cs = chunnksize\n",
    "    '''\n",
    "    This function extracts the content of the csv file containing the final data year-wise\n",
    "    cs = chunksize; if RAM is a constraint and there is lots of data in the csv files\n",
    "    '''\n",
    "    for a in pd.read_csv(f'Final_dataset/Final_data/{year}.csv', chunksize=cs):\n",
    "        df = pd.DataFrame(data=a)\n",
    "        mylist = df.values.tolist()\n",
    "    return mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3865e6c6-5c20-4bb6-93d7-9b76fd176664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_data = list()\n",
    "for year in range(start_year, end_year+1):\n",
    "    combined_data.extend(data_combine(year, 200))\n",
    "\n",
    "with open('Final_dataset.csv', 'w') as fp:\n",
    "    wr = csv.writer(fp, dialect='excel')\n",
    "    wr.writerow(headers)\n",
    "    wr.writerows(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057abea2-6ab9-4e79-81f8-423d2fe452d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
